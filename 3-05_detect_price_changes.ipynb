{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f238dce",
   "metadata": {},
   "source": [
    "### Price Changes\n",
    "If you're running an eCommerce website, it is helpful if you monitor the prices of your competitors. This script will help you do that.\n",
    "\n",
    "You'll need a CSV list of product URLs for this task. You can get your own by using Screaming Frog or using the same one I uploaded here in Github and in Moodle. Be aware that the more URLs you use the longer it will take to process.\n",
    "\n",
    "Some experience with BeautifulSoup and Chrome Dev Tools would be advantageous.\n",
    "\n",
    "In this example we're going to be scraping prices from https://www.theworkwearshack.co/. You may remember this site from eCommerce. The URL cvs is in Github and can be downloaded from Moodle.\n",
    "\n",
    "Credit @napo7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11e22951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the stuff we need\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import config\n",
    "import re\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9564fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the csv file where we are going to save the URLs and product prices. We will create the csv if it doesn't already exsist.\n",
    "def main():\n",
    "    file = pathlib.Path('saved_prices_workwearshack.csv') # Where your URL list is located.\n",
    "    if not file.exists():\n",
    "        write_data()\n",
    "    else:\n",
    "        compare_prices()\n",
    "        write_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e233f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're checking the URLS are valid. We'll use this later.\n",
    "def is_valid_url(str):\n",
    "    # Regex to check valid URL\n",
    "    regex = (\"((http|https)://)(www.)?\" +\n",
    "             \"[a-zA-Z0-9@:%._\\\\+~#?&//=]\" +\n",
    "             \"{2,256}\\\\.[a-z]\" +\n",
    "             \"{2,6}\\\\b([-a-zA-Z0-9@:%\" +\n",
    "             \"._\\\\+~#?&//=]*)\")\n",
    "\n",
    "    # Compile the ReGex\n",
    "    p = re.compile(regex)\n",
    "\n",
    "    # If the string is empty return false\n",
    "    if str is None:\n",
    "        return False\n",
    "\n",
    "    # Return if the string matched the ReGex\n",
    "    if re.search(p, str):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c959021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  https://www.theworkwearshack.co/index.php?route=product/product&product_id=3465\n",
      "0  https://www.theworkwearshack.co/index.php?rout...                             \n",
      "1  https://www.theworkwearshack.co/index.php?rout...                             \n"
     ]
    }
   ],
   "source": [
    "# Lets check our csv of URLs and make sure it is being read correctly.\n",
    "# We've only included 3 URLs to save time. The more you include, the more time it will take.\n",
    "url = \"input_workwearshack.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e2be192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the URLs and validate them\n",
    "def get_urls_from_file():\n",
    "    url_list = []\n",
    "    urls_file_path = 'input_workwearshack.csv'\n",
    "    df_urls = pd.read_csv(urls_file_path, header=None)\n",
    "\n",
    "    # validate that url is valid\n",
    "    for url in df_urls.values:\n",
    "        if is_valid_url(str(url)):\n",
    "            url_list.append(url)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c87dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where we are going to scrape the prices from each page\n",
    "# Be careful here. You many get bocked from websites if you do this too much! You may want to use your Shopify site. You may need to use the time module (see the web scraping script)\n",
    "def get_scraped_prices():\n",
    "    \n",
    "    urls_list = get_urls_from_file()\n",
    "    \n",
    "    # We're using the iPhone UA user agent https://webscraping.com/blog/User-agents/\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9B179 Safari/7534.48.3'}\n",
    "       \n",
    "    scraped_prices_dict = {}\n",
    "    for url in urls_list:\n",
    "        scraped_prices = []\n",
    "        page = requests.get(url[0], headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        # This part requires you to use Chrome Dev Tools and find the price in the HTML code. Some knowledge of BeautifulSoup would be helpful here.\n",
    "        # The code we are looking for: <span itemprop=\"price\">£4.78</span>\n",
    "        # We're using Regex to filter only the prices with a pound sign (£) from the text in the elements listed.\n",
    "        price = soup.find_all('span', itemprop=\"price\", text=re.compile(r'\\£'))\n",
    "\n",
    "        # An alternative scatter gun approach. This will find multiple prices on a page, possibly not from the product you're looking at but also related products.\n",
    "        # price = soup.find_all(['class', 'h1', 'h2', 'span', 'div', 'a', 'title', 'del', 'a', 'p'], text=re.compile(r'\\£'))\n",
    "        \n",
    "        # Save the URLs with their corresponding prices to the scraped_prices_dict{}\n",
    "        productprice = []\n",
    "        for x in re.findall('(\\£[0-9]+(\\.[0-9]+)?)', str(price)):\n",
    "            productprice.append(x[0])\n",
    "\n",
    "        price_digit = []\n",
    "        for x in re.findall('([0-9]+(\\.[0-9]+)?)', str(productprice)):\n",
    "            price_digit.append(x[0])\n",
    "        price_digit_unique = set(price_digit)\n",
    "\n",
    "        for price in price_digit_unique:\n",
    "            price = float(price)\n",
    "            scraped_prices.append(price)\n",
    "\n",
    "        scraped_prices.sort()\n",
    "        scraped_prices_dict.update({str(url): scraped_prices})\n",
    "\n",
    "    return scraped_prices_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "842bdabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"['https://www.theworkwearshack.co/index.php?route=product/product&product_id=3465']\": [4.78],\n",
       " \"['https://www.theworkwearshack.co/index.php?route=product/product&product_id=3470']\": [11.63],\n",
       " \"['https://www.theworkwearshack.co/index.php?route=product/product&product_id=3493']\": [7.73]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see what we scraped\n",
    "get_scraped_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "347deb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have done the first run, we can keep on running the script and comparing the two data sets to find any price changes\n",
    "def compare_prices():\n",
    "    # Get saved prices from file\n",
    "    prices_file_path = 'saved_prices_workwearshack.csv'\n",
    "    df_saved_prices = pd.read_csv(prices_file_path)  # header=1\n",
    "\n",
    "    # Get scraped prices\n",
    "    prices_values = list(get_scraped_prices().values())\n",
    "    price_keys = list(get_scraped_prices().keys())\n",
    "    df_scraped_prices = pd.DataFrame.from_dict(prices_values).transpose().fillna(0).reset_index(drop=True)\n",
    "    df_scraped_prices.columns = price_keys\n",
    "\n",
    "    # Compare saved prices to scraped prices\n",
    "    ne_stacked = (df_saved_prices != df_scraped_prices).stack()\n",
    "    for change in ne_stacked:\n",
    "        if change:\n",
    "            changed = ne_stacked[ne_stacked]\n",
    "            changed.index.names = ['ID', 'URL']\n",
    "            difference_locations = np.where(df_saved_prices != df_scraped_prices)\n",
    "            changed_from = df_saved_prices.values[difference_locations]\n",
    "            changed_to = df_scraped_prices.values[difference_locations]\n",
    "             \n",
    "            df_price_changes = pd.DataFrame({'Saved Price': changed_from,'Scraped Price': changed_to}, index=changed.index)\n",
    "            df_price_changes.to_csv('price-changes_workwearshack.csv', index=True, header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24a80929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "def write_data():\n",
    "    price_keys = get_scraped_prices().keys()\n",
    "    price_values = get_scraped_prices().values()\n",
    "\n",
    "    df_saved_prices = pd.DataFrame.from_dict(price_values).transpose().fillna(0).reset_index(drop=True)\n",
    "    df_saved_prices.columns = price_keys\n",
    "    df_saved_prices.to_csv('saved_prices_workwearshack.csv', index=False, header=True, mode='w')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573158dc",
   "metadata": {},
   "source": [
    "#### Hopefully that worked! How could you improve this?\n",
    "- Automate it, daily/weekly/monthly?\n",
    "- Create notifications. email, tweet it etc..\n",
    "- How can you handle different currencies?\n",
    "- Could you add the product name into the csv?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
