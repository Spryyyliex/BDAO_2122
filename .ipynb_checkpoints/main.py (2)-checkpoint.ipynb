{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff751902",
   "metadata": {},
   "source": [
    "### Price Changes\n",
    "If you're running an eCommerce website, it is helpful if you monitor the prices of your competitors. This script will help you do that.\n",
    "\n",
    "You'll need a CSV list of product URLs for this task. You can get your own by using Screaming Frog or using the same one I uploaded here in Github and in Moodle. Be aware that the more URLs you use the longer it will take to process.\n",
    "\n",
    "Some experience with BeautifulSoup and Chrome Dev Tools would be advantageous.\n",
    "\n",
    "In this example we're going to be scraping prices from https://www.classicfootballshirts.co.uk\n",
    "\n",
    "Credit @napo7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11e22951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the stuff we need\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import config\n",
    "import re\n",
    "import numpy as np\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9564fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the csv file where we are going to save the URLs and product prices\n",
    "def main():\n",
    "    file = pathlib.Path('saved_prices_classicfootballshirts.csv') # Where your URL list is located.\n",
    "    if not file.exists():\n",
    "        write_data()\n",
    "    else:\n",
    "  #      compare_prices()\n",
    "        write_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e233f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're checking the URLS are valid. We'll use this later.\n",
    "def is_valid_url(str):\n",
    "    # Regex to check valid URL\n",
    "    regex = (\"((http|https)://)(www.)?\" +\n",
    "             \"[a-zA-Z0-9@:%._\\\\+~#?&//=]\" +\n",
    "             \"{2,256}\\\\.[a-z]\" +\n",
    "             \"{2,6}\\\\b([-a-zA-Z0-9@:%\" +\n",
    "             \"._\\\\+~#?&//=]*)\")\n",
    "\n",
    "    # Compile the ReGex\n",
    "    p = re.compile(regex)\n",
    "\n",
    "    # If the string is empty return false\n",
    "    if str is None:\n",
    "        return False\n",
    "\n",
    "    # Return if the string matched the ReGex\n",
    "    if re.search(p, str):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b9715b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  https://www.classicfootballshirts.co.uk/2006-07-ac-milan-home-shirt-ronaldo-99-very-good-l-888728.html\n",
      "0  https://www.classicfootballshirts.co.uk/1993-9...                                                    \n",
      "1  https://www.classicfootballshirts.co.uk/2012-1...                                                    \n"
     ]
    }
   ],
   "source": [
    "# Lets check our csv of URLs and make sure it is being read\n",
    "url = \"input_classicfootballshirts.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e2be192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the URLs and validate the URL\n",
    "def get_urls_from_file():\n",
    "    url_list = []\n",
    "    urls_file_path = 'input_classicfootballshirts.csv'\n",
    "    df_urls = pd.read_csv(urls_file_path, header=None)\n",
    "\n",
    "    # validate that url is valid\n",
    "    for url in df_urls.values:\n",
    "        if is_valid_url(str(url)):\n",
    "            url_list.append(url)\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ac96e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where we are going to scrape the prices from each page\n",
    "def get_scraped_prices():\n",
    "    urls_list = get_urls_from_file()\n",
    "    # We're using the Chrome UA user agent https://webscraping.com/blog/User-agents/\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36)\"}\n",
    "    print = {}\n",
    "    scraped_prices_dict = {}\n",
    "    for url in urls_list:\n",
    "        scraped_prices = []\n",
    "        page = requests.get(url[0], headers=headers)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        # This part requires you to use Chrome Dev Tools and find the price in the HTML code. \n",
    "        # Some knowledge of BeautifulSoup would be helpful here.\n",
    "        # We're using Regex to filter only the prices with a pound sign (£) from the text in the elements listed.\n",
    "        price = soup.find(['span', {\"class\":\"price\"}], text=re.compile(r'\\£'))\n",
    "        \n",
    "        # An alternative scatter gun approach.\n",
    "        # price = soup.find_all(['class', 'h1', 'h2', 'span', 'div', 'a', 'title', 'del', 'a', 'p'], text=re.compile(r'\\£'))\n",
    "\n",
    "        # Save the URLs with their corresponding prices to the scraped_prices_dict{}\n",
    "        gbp = []\n",
    "        for x in re.findall('(\\£[0-9]+(\\.[0-9]+)?)', str(price)):\n",
    "            gbp.append(x[0])\n",
    "\n",
    "        price_digit = []\n",
    "        for x in re.findall('([0-9]+(\\.[0-9]+)?)', str(gbp)):\n",
    "            price_digit.append(x[0])\n",
    "        price_digit_unique = set(price_digit)\n",
    "\n",
    "        for price in price_digit_unique:\n",
    "            price = float(price)\n",
    "            scraped_prices.append(price)\n",
    "\n",
    "        scraped_prices.sort()\n",
    "        scraped_prices_dict.update({str(url): scraped_prices})\n",
    "\n",
    "    return scraped_prices_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5828e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only we have done the first run, we can keep on running the script and comparing the two data sets to find any price changes\n",
    "def compare_prices():\n",
    "    # Get saved prices from file\n",
    "    prices_file_path = 'saved_prices_classicfootballshirts.csv'\n",
    "    df_saved_prices = pd.read_csv(prices_file_path)  # header=1\n",
    "\n",
    "    # Get scraped prices\n",
    "    prices_values = list(get_scraped_prices().values())\n",
    "    price_keys = list(get_scraped_prices().keys())\n",
    "    df_scraped_prices = pd.DataFrame.from_dict(prices_values).transpose().fillna(0).reset_index(drop=True)\n",
    "    df_scraped_prices.columns = price_keys\n",
    "\n",
    "    # Compare saved prices to scraped prices\n",
    "    ne_stacked = (df_saved_prices != df_scraped_prices).stack()\n",
    "    for change in ne_stacked:\n",
    "        if change:\n",
    "            changed = ne_stacked[ne_stacked]\n",
    "            changed.index.names = ['ID', 'URL']\n",
    "            difference_locations = np.where(df_saved_prices != df_scraped_prices)\n",
    "            changed_from = df_saved_prices.values[difference_locations]\n",
    "            changed_to = df_scraped_prices.values[difference_locations]\n",
    "             \n",
    "            df_price_changes = pd.DataFrame({'Saved Price': changed_from,'Scraped Price': changed_to}, index=changed.index)\n",
    "            df_price_changes.to_csv('price-changes_classicfootballshirts.csv', index=True, header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24a80929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Excel\n",
    "def write_data():\n",
    "    price_keys = get_scraped_prices().keys()\n",
    "    price_values = get_scraped_prices().values()\n",
    "\n",
    "    df_saved_prices = pd.DataFrame.from_dict(price_values).transpose().fillna(0).reset_index(drop=True)\n",
    "    df_saved_prices.columns = price_keys\n",
    "    df_saved_prices.to_csv('saved_prices_classicfootballshirts.csv', index=False, header=True, mode='w')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cbae5",
   "metadata": {},
   "source": [
    "#### Hopefully that worked! How could you improve this?\n",
    "- Automate it, daily/weekly/monthly?\n",
    "- create notifications. email, tweet it etc.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
